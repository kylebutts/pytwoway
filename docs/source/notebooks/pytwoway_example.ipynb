{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTwoWay example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PyTwoWay to system path, do not run this\n",
    "# import sys\n",
    "# sys.path.append('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T23:38:19.123052Z",
     "start_time": "2021-01-15T23:38:18.565950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the PyTwoWay package \n",
    "# (Make sure you have installed it using pip install pytwoway)\n",
    "import pytwoway as tw\n",
    "import bipartitepandas as bpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate some data\n",
    "\n",
    "The package contains functions to simulate data. We use this here to keep things simple. If you have your own data, you can import it. Load it as a pandas dataframe and use it as an input.\n",
    "\n",
    "As you can see, we will need the following required columns in our data:\n",
    "\n",
    " - `i`: worker identifier\n",
    " - `j`: firm identifier\n",
    " - `y`: compensation\n",
    " - `t`: time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T23:38:20.546690Z",
     "start_time": "2021-01-15T23:38:20.254142Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimBipartite' object has no attribute 'sim_network'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For the example, we simulate data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sim_data \u001b[38;5;241m=\u001b[39m \u001b[43mbpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSimBipartite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim_network\u001b[49m()\n\u001b[1;32m      3\u001b[0m display(sim_data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimBipartite' object has no attribute 'sim_network'"
     ]
    }
   ],
   "source": [
    "# For the example, we simulate data\n",
    "sim_data = bpd.SimBipartite().simulate()\n",
    "display(sim_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TwoWay object using your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T23:38:23.409822Z",
     "start_time": "2021-01-15T23:38:23.405804Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need to specify a column dictionary to make sure columns are named correctly\n",
    "# You can also manually update column names yourself\n",
    "col_name_dict = {\n",
    "    'i': 'i',    # Specify the column name for the worker identifier \n",
    "    'j': 'j',    # Specify the column name for the firm identifier \n",
    "    'y': 'y',  # Specify the column name for the compensation variable\n",
    "    't': 't'   # Specify the column name for the time variable\n",
    "}\n",
    "\n",
    "# Create the TwoWay object that will do all the heavy lifting\n",
    "tw_net = tw.TwoWay(data=sim_data, formatting='long', col_dict=col_name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional Parameters ##\n",
    "clean_params = {\n",
    "    'connectedness': 'connected', # When computing largest connected set of firms:\n",
    "        # If 'connected', keep observations in the largest connected set of firms;\n",
    "        # If 'biconnected', keep observations in the largest biconnected set of firms;\n",
    "        # If None, keep all observations\n",
    "    'i_t_how': 'max', # How to handle worker-year duplicates\n",
    "        # If 'max', keep max paying job;\n",
    "        # If 'sum', sum over duplicate worker-firm-year observations,\n",
    "            # then take the highest paying worker-firm sum;\n",
    "        # If 'mean', average over duplicate worker-firm-year observations,\n",
    "            # then take the highest paying worker-firm average.\n",
    "        # Note that if multiple time and/or firm columns are included\n",
    "            # (as in event study format), then data is converted to long,\n",
    "            # cleaned, then reconverted to its original format\n",
    "    'data_validity': True, # If True, run data validity checks; much faster if set to False\n",
    "    'copy': False # If False, avoid copy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_net.prep_data(\n",
    "    collapsed=True, # If True, run estimators on collapsed data\n",
    "    user_clean=clean_params,\n",
    "    he=False # If True, compute largest biconnected set of firms for heteroskedastic correction\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can run the CRE estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T21:42:51.485812Z",
     "start_time": "2020-12-22T21:42:51.144952Z"
    }
   },
   "outputs": [],
   "source": [
    "## Optional Parameters ##\n",
    "cre_params = {\n",
    "    'ncore': 1, # Number of cores to use\n",
    "    'ndraw_tr': 5, # Number of draws to use in approximation for traces\n",
    "    'ndp': 50, # Number of draw to use in approximation for leverages\n",
    "    'out': 'res_cre.json', # Outputfile where results are saved\n",
    "    'posterior': False, # If True, compute posterior variance\n",
    "    'wo_btw': False # If True, sets between variation to 0, pure RE\n",
    "}\n",
    "cluster_params = {\n",
    "    'measures': bpd.measures.cdfs(\n",
    "        cdf_resolution=10, # How many values to use to approximate the cdfs\n",
    "        measure='quantile_all' # How to compute the cdfs (\n",
    "                               # 'quantile_all' to get quantiles from entire set of data,\n",
    "                                    # then have firm-level values between 0 and 1;\n",
    "                               # 'quantile_firm_small' to get quantiles at the firm-level\n",
    "                                    # and have values be compensations if small data;\n",
    "                               # 'quantile_firm_large' to get quantiles at the firm-level\n",
    "                                    # and have values be compensations if large data,\n",
    "                                    # note that this is up to 50 times slower than\n",
    "                                    # 'quantile_firm_small' and should only be used\n",
    "                                    # if the dataset is too large to copy into a dictionary\n",
    "    ),\n",
    "    'grouping': bpd.grouping.kmeans( # Read more at\n",
    "                                  # https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "        n_clusters=10,\n",
    "        init='k-means++',\n",
    "        n_init=500,\n",
    "        max_iter=300,\n",
    "        tol=0.0001,\n",
    "        precompute_distances='deprecated',\n",
    "        verbose=0,\n",
    "        random_state=None,\n",
    "        copy_x=True,\n",
    "        n_jobs='deprecated',\n",
    "        algorithm='auto'\n",
    "    ),\n",
    "    'stayers_movers': None, # If None, clusters on entire dataset;\n",
    "                         # If 'stayers', clusters on only stayers\n",
    "                         # If 'movers', clusters on only movers\n",
    "    't': None, # If None, clusters on entire dataset\n",
    "            # If int, gives period in data to consider (only valid for non-collapsed data)\n",
    "    'weighted': True, # If True, weight firm clusters by firm size\n",
    "                        # (if a weight column is included, firm weight is computed using this column;\n",
    "                        # otherwise, each observation has weight 1)\n",
    "    'dropna': False # If True, drop observations where firms aren't clustered;\n",
    "                 # If False, keep all observations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster\n",
    "tw_net.cluster(**cluster_params)\n",
    "# Estimate the cre model\n",
    "tw_net.fit_cre(user_cre=cre_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also run the FE estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, compute largest biconnected set of firms for heteroskedastic correction\n",
    "tw_net.prep_data(\n",
    "    collapsed=True, # If True, run estimators on collapsed data\n",
    "    user_clean=clean_params,\n",
    "    he=True # If True, compute largest biconnected set of firms for heteroskedastic correction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional Parameters ##\n",
    "fe_params = {\n",
    "    'ncore': 1, # Number of cores to use\n",
    "    'batch': 1, # Batch size to send in parallel\n",
    "    'ndraw_pii': 50, # Number of draws to use in approximation for leverages\n",
    "    'levfile': '', # File to load precomputed leverages\n",
    "    'ndraw_tr': 5, # Number of draws to use in approximation for traces\n",
    "    'he': True, # If True, compute heteroskedastic correction\n",
    "    'out': 'res_fe.json', # Outputfile where results are saved\n",
    "    'statsonly': False, # If True, return only basic statistics\n",
    "    'feonly': False, # If True, compute only fixed effects and not variances\n",
    "    'Q': 'cov(alpha, psi)', # Which Q matrix to consider. Options include 'cov(alpha, psi)' and 'cov(psi_t, psi_{t+1})'\n",
    "    'seed': None # NumPy RandomState seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Estimate the fixed effect decomposition\n",
    "tw_net.fit_fe(user_fe=fe_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we can investigate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T21:42:51.498849Z",
     "start_time": "2020-12-22T21:42:51.489723Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(tw_net.summary_cre())\n",
    "display(tw_net.summary_fe())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbsphinx-toctree": {
   "hidden": true,
   "maxdepth": 1,
   "titlesonly": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
